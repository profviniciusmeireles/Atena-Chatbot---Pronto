#pip install google-generativeai
#pip install streamlit

import os
import streamlit as st
import google.generativeai as genai
import pandas as pd
from dotenv import load_dotenv


# Carrega as vari√°veis de ambiente do arquivo .env
load_dotenv()

# Configura a chave de API do Google Cloud
os.environ['GOOGLE_API_KEY'] = os.getenv("GEMINI_API_KEY")

# Configura√ß√£o do genai com a chave de API do Google Cloud
genai.configure(api_key = os.environ['GOOGLE_API_KEY'])


#LLM (large language model) do Gemini 
llm = genai.GenerativeModel('gemini-1.0-pro')

#Configura√ß√£o da P√°gina do APP:

st.set_page_config(page_title="Atena ChatBot", page_icon= "ü§ñ") 

# Style: OCULTAR APPS
with open('style.css')as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html = True)



#Streamlit oferece v√°rios elementos que facilitam a cria√ß√£o de interfaces gr√°ficas do usu√°rio para chatbots. 
#Um dos elementos que usaremos ser√° o estado da sess√£o (session_state) para armazenar o hist√≥rico do chat para ele aparecer no nosso aplicativo. 
#Primeiro, verificamos se o estado da sess√£o tem a chave messages indicando que a intera√ß√£o j√° foi iniciada. Se n√£o tiver, ela ser√° inicializada como uma lista contendo apenas uma frase inicial do bot (content).
def main():

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role":"assistant",
                "content":"Ol√°! o que posso te ajudar?"
            }
        ]

    #Depois, adicionamos um for loop para iterar pela lista com o hist√≥rico do chat e exibir cada mensagem no cont√™iner de mensagens do nosso aplicativo.

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
    #Em seguida, usamos um elemento do tipo chat_input, para receber as quest√µes do usu√°rio do aplicativo.  
    query = st.chat_input("Oi, me pergunte qualquer coisa")

       
    #Na sequ√™ncia, definimos uma fun√ß√£o que acessa o LLM atrav√©s da Gemini API e obt√©m a resposta para as consultas do usu√°rio. 
    #A fun√ß√£o tamb√©m ser√° usada para armazenar as consultas e as respostas da intera√ß√£o.
    def function_llm(query):
        ''' Fun√ß√£o que acessa o LLM, gera a resposta e salva a intera√ß√£o.'''

        # acessa o LLM para gerar uma resposta
        response = llm.generate_content(query)

        # Mostra as mensagens do bot
        with st.chat_message("assistant"):
            st.markdown(response.text)
        
        # Salva as mensagens do usu√°rio
        st.session_state.messages.append(
            {
                "role":"user",
                "content": query
            }
        )

        # Salva as mensagens do bot
        st.session_state.messages.append(
            {
                "role":"assistant",
                "content": response.text
            }
        )

    # Ap√≥s isso, precisamos definir um comando para verificar se existe uma mensagem do usu√°rio no nosso aplicativo. 
    #Em caso afirmativo, ela √© exibida e a fun√ß√£o definida acima √© chamada. Esse trecho encerra nosso aplicativo.  
    if query:
        # Mostra a mensagem do usu√°rio
        with st.chat_message("user"):
            st.markdown(query)
            
        # chama a fun√ß√£o que acessa o LLM
        function_llm(query)
        # Exibir o hist√≥rico de mensagens no sidebar
    with st.sidebar:
        st.write("Recentes")
        for msg in st.session_state.messages:
            if msg['role'] == 'user':
                st.info(f"‚úâ {msg['content']}")
           
#fun√ß√£o para limpar historico do chat
def clear_chat_history():
    st.session_state.messages = [
        {
        "role": "assistant", 
        "content": "Ol√°! o que posso te ajudar?"
        }
    ]


st.sidebar.button("‚ûï Nova conversa", on_click=clear_chat_history)

#st.logo("img/logo.png")
      
if __name__ == "__main__":
    main() 